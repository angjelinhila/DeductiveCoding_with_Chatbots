# -*- coding: utf-8 -*-
"""Qualitative Coding Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJ5Gl0rQH7Q2fGBQbVaEYQ3TeIPUbhYU
"""

#dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
from pylab import rcParams
from sklearn.model_selection import train_test_split
from sklearn import metrics

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import scale

#Classification dependencies with Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

#Parametric Classifiers


from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import CategoricalNB

from google.colab import files
uploaded = files.upload()

data = pd.read_csv('SupremeCourt_cases_.csv')
data.head()

y = data[['majortopic']]

X = data[['summary']]

data.shape

def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop:
            final_text.append(i.strip())
    return " ".join(final_text)

import nltk
import string
from nltk.corpus import stopwords
import plotly.graph_objects as go
import plotly.express as px
nltk.download("popular")
stop = set(stopwords.words('english'))

data['summary'] = data['summary'].astype(str)

type('majortopic')

#data['majortopic'] = data['majortopic'].astype(int)

data['majortopic'] = pd.to_numeric(data['majortopic'], errors='coerce').fillna(-1).astype(int)

data = data[data['majortopic'] != -555]



punctuation = list(string.punctuation)
stop.update(punctuation)

data['summary']=data['summary'].apply(remove_stopwords)

#size of the vocabulary in the model to be trained within an embedding layer
vocab_size = 10000
#number of embedding dimensions; this is something that we can vary and which is vocab-dependent
embedding_dim = 16
#the maxisum length of a single sentence
max_length = 300
#type of truncation and padding
trunc_type='post'
padding_type='post'
#the string identifier for our model to replace out-of-vocabulary tokens with
oov_tok = "<OOV>"
#number of sentences/samples we're using to train the model
training_size = 20000
#how many iterations of the model are we running. An epoch consists of the processing our entire trainig sets
epochs = 11

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Let's do the customery train_test split for both our headlines/samples as well as labels (x = headlines, y = labels)

x_train, x_test, y_train, y_test = train_test_split(data['summary'], data['majortopic'], train_size = 0.9, random_state = 73)

#Some further and important pre-processing

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(x_train)
word_index = tokenizer.word_index

#Let's now convert our tokenized data into sequences with assigned number values

#Padding: to train a  neural net you need all your data to be in the same shape, in other words all sentences
#need to be of the same size. Padding allows to make the all the same size by adding zeros.
#Padding can be pre or post sentence.
#maxlen: lets us put a limit on a how big the longest sentence can be
#Truncating: let's us decide whether our maxlen truncation happens from the beginning or end of sentence

#Out of vocabulary tokens: an identifier that acts as a placeholder for words not included in our model

train_sequences = tokenizer.texts_to_sequences(x_train)
x_train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(x_test)
x_test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

x_train_padded

#Building the model

# backend.clear_session Resets all state generated by Keras.
#A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator.
#A pseudorandom number generator's number sequence is completely determined by the seed:
#thus, if a pseudorandom number generator is reinitialized with the same seed, it will produce the same sequence of numbers.

#visualize embedding: project.tensorflow.org

#Recurrent Neural Networks: make use of feedback (output fed back as input)
#After each input the system creates a new state where the last state is stored as delay
#Recurrent cells usually have their own layer, where a single recurrent cell has more than 1 neural network in it
#Because of the hidden state, each output depends on the new input as well as the delay/hidden state
#State stored as 1d tensor
#Vanishing + exploding gradient problems for RNNS
#Problem: RNNs context rentention fades with each iteration since it only remembers the last state
#Solution: Create a state that persists across iterations

#Dropout Layers: technique where randomly selected neurons are ignored during training
#Dropout rate: 0.3 = 30% of connections will be randomly ignored with every iteration

#tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout = 0.3 , dropout = 0.3, return_sequences = True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,recurrent_dropout = 0.1 , dropout = 0.1)),
    tf.keras.layers.Dense(512, activation = "relu"),
    tf.keras.layers.Dense(24, activation = "softmax")
])

#model = tf.keras.Sequential([
#tf.keras.layers.Embedding(vocab_size, embedding_dim),
#tf.keras.layers.GlobalAveragePooling1D(),
#tf.keras.layers.Dense(24, activation='relu'),
#tf.keras.layers.Dense(1, activation='sigmoid')
#])

#GlobalAveragePooling1D() --->
model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])

print("Unique labels before training:", data['majortopic'].unique())



history = model.fit(x_train_padded,y_train, batch_size = 128, epochs = epochs, validation_data = (x_test_padded, y_test))

pred = (model.predict(x_test_padded)) #.astype("int32")

#pred = np.argmax(model.predict(x_test_padded), axis=1)

pred

import numpy as np
y_pred_classes = np.argmax(pred, axis=1)

y_pred_classes

y_test

print(classification_report(y_test, y_pred_classes))

report = classification_report(y_test, y_pred_classes)

cm = confusion_matrix(y_test, y_pred_classes)

pred_df = pd.DataFrame(y_pred_classes, columns=['predictions'])

report = pd.DataFrame(classification_report(y_test, y_pred_classes, output_dict=True)).transpose()
report.to_csv('classification_report.csv')
from google.colab import files
files.download('classification_report.csv')

results_df = pd.DataFrame({
    'Actual_Labels': y_test,    # Replace with the variable containing actual test labels
    'Predicted_Labels': y_pred_classes
})

results_df.shape

results_df.to_csv('actual_vs_predicted.csv', index=False)

from google.colab import files
files.download('actual_vs_predicted.csv')

sklearn.metrics.cohen_kappa_score(y_test, y_pred_classes, labels=None, weights=None)